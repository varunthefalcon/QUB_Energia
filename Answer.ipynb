{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e48484b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.exceptions.ConvergenceWarning('ignore')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4e63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # used for plot interactive graph. \n",
    "from sklearn.model_selection import train_test_split # to split the data into two parts\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # to apply the Logistic regression\n",
    "from sklearn.model_selection import RandomizedSearchCV  # Randomized search on hyper parameters.\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "from scipy.stats import boxcox\n",
    "from sklearn import metrics # for the check the error and accuracy of the model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67937c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading and concatinating customer data\n",
    "\n",
    "raw_1 = pd.read_excel('D:/UK/assignment/DSA8023/WB1_Energia_Challenge_March_2023_Data.xlsx', sheet_name=1)\n",
    "raw_2 = pd.read_excel('D:/UK/assignment/DSA8023/WB2_Energia_Challenge_March_2023_Data.xlsx', sheet_name=1)\n",
    "raw_d = pd.concat([raw_1, raw_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26320dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "\n",
    "is_zero_bill_value_allowed = False\n",
    "\n",
    "billing_months = ['bill_1_2021', 'bill_2_2021',\n",
    "       'bill_3_2021', 'bill_4_2021', 'bill_5_2021', 'bill_6_2021',\n",
    "       'bill_1_2022', 'bill_2_2022', 'bill_3_2022', 'bill_4_2022',\n",
    "       'bill_5_2022', 'bill_6_2022', 'bill_1_2023']\n",
    "\n",
    "categorical_columns = ['title', 'mosaicType', 'agedBand', 'saStatus', 'signedUpGroup']\n",
    "\n",
    "# based on the fact that ifn one reading is missed for a particular month, the reading will be accumulated to next month.\n",
    "# the reading is averaged between missing and accumulated months.\n",
    "# only one missed month is accepted thus eleminating rows with consecutive zeros\n",
    "# end result will have no zero readings\n",
    "def average_missing_months(x):\n",
    "    for index, month in enumerate(billing_months[:-1]):\n",
    "        next_month = billing_months[index+1]\n",
    "        if x[month] == 0:\n",
    "            if(x[next_month] != 0):\n",
    "                x[month] = x[next_month] / 2\n",
    "                x[next_month] = x[next_month] / 2\n",
    "            else:\n",
    "                break\n",
    "    return x\n",
    "\n",
    "def checkCateoricalInfo(data, cols):\n",
    "    for col in cols:\n",
    "        unique = data[col].unique()\n",
    "        print('{}({}) - {}'.format(col, len(unique), unique));\n",
    "        print()\n",
    "\n",
    "def scale_data_standard_scaler(data, feature_cols):\n",
    "    scaled_data = data.copy()\n",
    "    features = scaled_data[feature_cols]\n",
    "    scaler = StandardScaler().fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    scaled_data[feature_cols] = features\n",
    "    return scaled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_d.copy()\n",
    "\n",
    "checkCateoricalInfo(df, categorical_columns)\n",
    "\n",
    "le = LabelEncoder()\n",
    " \n",
    "# keeping nan values in title, agedband as they have relatively less effect on EV classification\n",
    "df['title'] = le.fit_transform(df['title'])\n",
    "df['mosaicType'] = le.fit_transform(df['mosaicType'])\n",
    "df['agedBand'] = le.fit_transform(df['agedBand'])\n",
    "df['signedUpGroup'] = le.fit_transform(df['signedUpGroup'])\n",
    "df['saStatus'] = le.fit_transform(df['saStatus'])\n",
    "\n",
    "\n",
    "# manually assigning numeric values to EV related categories\n",
    "rename_props = { 'EV': {'N': 0, 'Y': 1},\n",
    "                  'EV_New_or_Old': {'Non-EV Customers': 0, 'Switched to EV': 1, 'New EV Customer': 1}} \n",
    "\n",
    "df = df.replace(rename_props)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9387d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeFeatureCount(dataset, feature = 'EV'):\n",
    "    totalData = len(dataset)\n",
    "    hasFeature = dataset[feature].sum()\n",
    "    \n",
    "#     print(totalData, has)\n",
    "    hasNoFeature = totalData - hasFeature\n",
    "\n",
    "    hasFeature_per = round(hasFeature/totalData * 100, 2)\n",
    "    hasNoFeature_per = round(hasNoFeature/totalData * 100, 2)\n",
    "\n",
    "    plt.figure()\n",
    "    sns.countplot(data = dataset, x=feature)\n",
    "    plt.annotate('No {}: {}'.format(feature, hasNoFeature), xy=(-0.3, 15000), xytext=(-0.3, 3000), size=12)\n",
    "    plt.annotate('{}: {}'.format(feature, hasFeature), xy=(0.7, 15000), xytext=(0.9, 3000), size=12)\n",
    "    plt.annotate(str(hasNoFeature_per)+\" %\", xy=(-0.3, 15000), xytext=(-0.1, 8000), size=12)\n",
    "    plt.annotate(str(hasFeature_per)+\" %\", xy=(0.7, 15000), xytext=(0.9, 8000), size=12)\n",
    "    plt.show()\n",
    "\n",
    "def plot_hist_plots(t_data, t_features, fig_size= 8):\n",
    "    f = plt.figure(figsize=(fig_size,fig_size))\n",
    "    for i, feature in enumerate(t_features):\n",
    "        t = f.add_subplot(round(len(t_features)/3) + 1,3, i+1)\n",
    "        sns.histplot(t_data[feature])\n",
    "        t.title.set_text(feature)\n",
    "    plt.show();\n",
    "\n",
    "def plot_corr_plot(ds):\n",
    "    corr = ds.corr(numeric_only=True) # .corr is used to find corelation\n",
    "    f,ax = plt.subplots(figsize=(8, 7))\n",
    "    sns.heatmap(corr, cbar = True,  square = True, annot = False, fmt= '.1f', \n",
    "                xticklabels= True, yticklabels= True\n",
    "                ,cmap=\"coolwarm\", linewidths=.5, ax=ax)\n",
    "    plt.title('CORRELATION MATRIX - HEATMAP', size=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ef5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(average_missing_months, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initial data cleaning\n",
    "\n",
    "# deleting date related \n",
    "cols_to_delete = ['StartDate','ContractStartDateEV','contractStartDate','contractEndDate','saStatus', 'accountID']\n",
    "if cleaned_data.columns.isin(cols_to_delete).any():\n",
    "    df = df.drop(columns=cols_to_delete)\n",
    "\n",
    "if is_zero_bill_value_allowed:\n",
    "    df['zero_count'] = df[billing_months].isin([0]).sum(axis=1)\n",
    "    cleaned_data = df.copy()\n",
    "else:\n",
    "    cleaned_data = df[~df[billing_months].isin([0]).any(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "visualizeFeatureCount(cleaned_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4200aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heat map of correlation features\n",
    "plot_corr_plot(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4fdc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of billing features before transforming\n",
    "plot_hist_plots(cleaned_data, cleaned_data.columns, fig_size=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying log transformation to fix right skewness in billing period distribution\n",
    "# sqrt_t_data = cleaned_data.copy()\n",
    "\n",
    "# for month in billing_months:\n",
    "#     sqrt_t_data[month] = np.sqrt(sqrt_t_data[month])\n",
    "\n",
    "# plot_hist_plots(sqrt_t_data, billing_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf50f23",
   "metadata": {},
   "source": [
    "# perform logistic regression\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "def perform_sampling(x_train, y_train, y_test, sampling):\n",
    "    print('Training set: {}'.format(Counter(y_train)))\n",
    "    print('Testing set: {}'.format(Counter(y_test)))\n",
    "    \n",
    "    if sampling == 'under':\n",
    "        from imblearn.under_sampling import NearMiss \n",
    "        sampler = NearMiss(version=1, n_neighbors=3)\n",
    "    elif sampling == 'over':\n",
    "        from imblearn.over_sampling import SMOTE \n",
    "        sampler = SMOTE(random_state=42)\n",
    "\n",
    "    x_train, y_train = sampler.fit_resample(x_train, y_train)\n",
    "    print('Training set: {}'.format(Counter(y_train)))\n",
    "    print('Testing set: {}'.format(Counter(y_test)))\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def perform_logistic_reg(data, class_col, sampling=None, solver='lbfgs'):\n",
    "    x = data.drop(class_col, axis=1)  \n",
    "    y = data[class_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20, stratify=y, random_state=42)\n",
    "    \n",
    "    if sampling:\n",
    "        X_train, y_train = perform_sampling(X_train, y_train,y_test,  sampling)\n",
    "\n",
    "    LR = LogisticRegression(C=0.0005, random_state=0, solver=solver)\n",
    "    LR.fit(X_train, y_train)\n",
    "    y_pred = LR.predict(X_test)\n",
    "    get_metrics(y_pred, X_test, y_test, LR, x, y, sampling)\n",
    "    return LR\n",
    "\n",
    "def get_metrics(y_pred, X_test, y_test, LR, x, y, sampling):\n",
    "    print('Accuracy:', metrics.accuracy_score(y_pred,y_test))\n",
    "\n",
    "    ## 5-fold cross-validation \n",
    "    cv_scores =cross_val_score(LR, x, y, cv=5)\n",
    "\n",
    "    # Print the 5-fold cross-validation scores\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print()\n",
    "    print(\"Average 5-Fold CV Score: {}\".format(round(np.mean(cv_scores),4)),\n",
    "          \", Standard deviation: {}\".format(round(np.std(cv_scores),4)))\n",
    "\n",
    "    plt.figure(figsize=(4,3))\n",
    "    ConfMatrix = confusion_matrix(y_test,LR.predict(X_test))\n",
    "    sns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n",
    "                xticklabels = ['No EV', 'EV'], \n",
    "                yticklabels = ['No EV', 'EV'])\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(\"Confusion Matrix - Logistic Regression - {}\".format(sampling));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f544cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR2 = perform_logistic_reg(cleaned_data, class_col='EV', sampling='over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcdc53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_data = scale_data_standard_scaler(cleaned_data, billing_months)\n",
    "# LR4 = perform_logistic_reg(scaled_data, class_col='EV', sampling='over' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39123423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.copy()\n",
    "\n",
    "# x = df[df.EV == 1]\n",
    "\n",
    "EV = df['EV']\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x.head()\n",
    "\n",
    "x= x.drop(columns=['EV'])\n",
    "\n",
    "x['EV_predicted'] = LR2.predict(x)\n",
    "x['EV'] = \n",
    "# visualizeFeatureCount(x, feature='EV_predicted')    \n",
    "\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427fca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
